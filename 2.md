# SERL

# Components

## RLPD

*   **高样本效率**：RLPD是一种离策略（off-policy）的Actor-Critic算法，基于Soft Actor-Critic (SAC)改进而来 。它支持高“更新-数据比率”（UTD），即每与环境交互一次，就在内部进行多次梯度更新，从而充分利用来之不易的真实世界数据 。
*   **融合先验数据**：该算法能够轻松地将人类演示等先验数据整合进训练过程中 。具体做法是在每次训练时，从先验数据（如演示）和在线采集的数据中各采样一半，组成一个训练批次 。这使得策略可以从一个较高的起点开始学习，极大加速了收敛。

## 奖励函数设计

*   **二元分类器奖励**：用户只需收集少量任务成功和失败状态的图像，训练一个二元分类器 。在RL训练中，该分类器会判断当前状态是否成功，并输出一个概率，奖励函数则被设为该成功概率的对数（`r(s) = log p(e|s)`），从而引导策略学习 。
*   **对抗性训练（VICE）**：为了防止策略找到“欺骗”分类器的捷径（即看起来成功但实际未成功的状态），SERL支持使用VICE方法 。该方法会将策略访问过的所有状态都作为负样本来更新分类器，形成一种策略（生成器）和奖励分类器（判别器）之间的对抗性训练

## 免重置的持续训练框架

系统同时训练两个独立的RL智能体：

*   **Forward Policy**：学习完成指定任务（例如，将物体从A点移动到B点） 。
*   **Backward Policy**：学习将任务“撤销”，即把环境恢复到初始状态（例如，将物体从B点移回A点） 。

通过交替执行这两个策略，机器人可以在无人干预的情况下持续进行“尝试-重置”的循环，实现7x24小时的自主训练

![\<img alt="" data-attachment-key="2HK5VVEB" width="413" height="334" src="attachments/2HK5VVEB.png" ztype="zimage"> | 413](attachments/2HK5VVEB.png)

**Actor-learner**: Actor（执行器）节点负责与机器人环境交互并生成动作，而Learner（学习器）节点负责在后台进行策略网络的更新计算 。

这种分离设计确保了机器人的控制频率可以稳定在固定值，这对于需要快速响应的动态任务至关重要 。同时，它也减少了因计算等待造成的总训练时间。

## For contact-rich tasks

对于PCB插入这类需要与环境进行精密接触的任务，SERL提出并集成了一种特别有效的**阻抗控制器设计**。

*   **分层控制**：RL策略以较低频率（如10Hz）输出目标位姿，而底层的实时控制器（文中是PID）则以高频（如1kHz）跟踪该目标 。
*   **实时层误差限制**：传统方法可能会因RL策略输出的目标离当前位置太远而产生巨大的接触力，导致硬件损坏 。SERL的关键创新在于，它**不在低频的RL策略层限制动作大小，而是在高频的实时控制器层限制目标位姿与当前位姿的误差**`e` 。

![\<img alt="" data-attachment-key="YU5KZLRS" width="432" height="416" src="attachments/YU5KZLRS.png" ztype="zimage"> | 432](attachments/YU5KZLRS.png)

如图，因为高频控制器+误差严格限制的关系，可以做到柔顺并不牺牲整体运动效率

## 相对坐标系

**相对观测**：机器人的本体感知信息（如末端执行器位姿）是相对于**本回合初始时刻**的末端位姿来表示的，而不是相对于固定的世界坐标系 。

1.  **相对动作**：策略输出的动作（如6D速度）是相对于**当前时刻**的末端执行器坐标系 。
2.  **效果**：这种“以自我为中心”的表示方法，相当于在训练中模拟了目标的移动，使得策略学会的是一种相对运动关系，从而在测试时即使目标位置被扰动，策略也能成功完成任务 。
