# Overview of HIL-SERL

![\<img alt="" data-attachment-key="BDBE4B27" width="820" height="493" src="attachments/BDBE4B27.png" ztype="zimage"> | 820](attachments/BDBE4B27.png)

# Process for training HIL-SERL

![\<img alt="" data-attachment-key="B8MQXZ68" width="802" height="394" src="attachments/B8MQXZ68.png" ztype="zimage"> | 802](attachments/B8MQXZ68.png)

分布式架构，异步通信：

*   Actor Process

    *   直接与机器人、环境交互的部分，加载策略并执行， 同时回传采集到的数据到Replay Buffer. 此过程可由人类操作手通过SpaceMouse等设备随时介入。

*   Learner Process

    *   从经验回放池中均匀采样数据，使用RLPD算法更新策略网络和价值网络的参数

*   Replay Buffer

    *   Demo Buffer

        *   存放20-30条人类专家演示数据，以及干扰数据

    *   RL Buffer

        *   存放机器人自主执行策略时产生的数据

    *   learner将在两个池中等量采样数据进行训练

# Training Process

*   **First**, we select cameras that are most suitable for the task. For all cameras, we perform image cropping to focus on the area of interest and resize the images to 128x128 for the neural network to process.
*   **Next**, we collect data to train the reward classifier. Additionally, we may collect extra data to address any false negative and false positive issues with the reward classifier. The trained reward classifier generally achieves an accuracy of greater than 95% in the evaluation data set.（训练奖励二元分类器）
*   We **then** collect 20-30 trajectories of human demonstrations solving the tasks and use them to initialize the offline demo replay buffer. For each task, we either script a robot reset motion or let the human operator manually reset the task at the beginning of each trajectory, such as the USB pick-insertion task. （填充演示经验池）
*   **Finally**, we start the policy training process. During this phase, human interventions may be provided to the policy if necessary, until the policy converges. It’s also important to note that we should avoid persistently providing long sparse interventions that lead to task successes. Such an intervention strategy will cause the overestimation of the value function, particularly in the early stages of the training process; which can result in unstable training dynamics. （在线人机强化学习）

# Summary

## 设计理念：结合模仿学习和强化学习的优点

*   通过初始的人类演示和训练过程中的实时纠正，为机器人策略提供高质量的初始引导和关键时刻的指导，减少机器人进行无效探索所需的时间 。
*   **强化学习的优化**：利用RL算法强大的优化能力，使策略不仅能学习人类的操作，还能通过自主试错和奖励驱动，探索出比人类演示更优、更快、更稳定的解决方案 。

## 真实物理世界运行使用技术

### 二元的稀疏奖励函数

*   RL传统的reward设计方法对不同task的复用性差，而且设计时间经验成本高。为了解决这个问题，这篇文章首先通过遥操作收集少量成功和失败状态的图像数据（约5分钟），训练一个二元分类器 。在训练中，只有当任务完成时，分类器才会给予一个正奖励，否则奖励为零 。

### 预训练视觉主干网络

*   采用了在ImageNet上预训练好的ResNet-10模型作为视觉特征提取器 。这使得系统可以更快地学习到与任务相关的视觉特征，显著提升了样本效率和稳定性 。

### **Core**: 人机循环的干预与学习机制

*   在训练过程中，人类操作员监控机器人的行为。当机器人陷入困境或执行错误动作时，操作员可以随时介入并提供纠正性操作 。这些干预数据会被同时存入演示池和RL池，用于后续的离策略学习 。随着策略的改进，人类干预的频率会逐渐降低，最终趋近于零 。

### Ego-centric Formulation

*   从本质上讲，在每个训练事件的开始时，机器人的最终效果的姿势在工作区的预定区域内均匀地随机分配。

### 阻抗控制器

### 分离的抓取器控制

*   机械爪开合的任务，系统并没有将离散的抓取动作（开、合、保持）与连续的手臂运动统一建模 。而是为抓取动作单独训练了一个评判家网络（Critic Network），使用DQN算法来决定在当前状态下应该执行哪种抓取动作 。

# 分类

*   异步安全RL架构：安全探索+离线和在线结合 的方法
*   限制探索：人类在危险时刻接管
*   底层安全层：阻抗控制
*   reward shaping via demonstrations: 解决reward engineering 难的问题
*   Representation choices for safe generalization: 减少策略对绝对坐标的依赖
*   Modular architectures for safety: 把末端爪子的抓取策略分开
